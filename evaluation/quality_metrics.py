"""
è´¨é‡è¯„ä¼°å·¥å…·
è‡ªåŠ¨è®¡ç®—AIMEé¢˜ç›®çš„å¤šæ ·æ€§æŒ‡æ ‡å’Œè´¨é‡æŒ‡æ ‡
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re


class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, problems_file: str = "output/stage4_improved/improved_problems.json"):
        self.problems_file = Path(problems_file)
        self.problems = self.load_problems()
    
    def load_problems(self) -> List[Dict[str, Any]]:
        """åŠ è½½é¢˜ç›®"""
        if not self.problems_file.exists():
            print(f"âŒ é¢˜ç›®æ–‡ä»¶ä¸å­˜åœ¨: {self.problems_file}")
            return []
        
        with open(self.problems_file, 'r', encoding='utf-8') as f:
            problems = json.load(f)
        
        return problems
    
    def calculate_all_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ‰€æœ‰è´¨é‡æŒ‡æ ‡"""
        if not self.problems:
            return {}
        
        metrics = {
            'basic_stats': self.get_basic_statistics(),
            'diversity': self.calculate_diversity_metrics(),
            'difficulty': self.analyze_difficulty_distribution(),
            'topic_coverage': self.analyze_topic_coverage(),
            'answer_distribution': self.analyze_answer_distribution(),
            'solution_quality': self.analyze_solution_quality(),
            'similarity': self.calculate_similarity_metrics(),
        }
        
        return metrics
    
    def get_basic_statistics(self) -> Dict[str, Any]:
        """åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        total = len(self.problems)
        
        # ç»Ÿè®¡æœ‰è§£ç­”çš„é¢˜ç›®
        with_solution = sum(1 for p in self.problems if 'solution' in p and p['solution'])
        
        # ç»Ÿè®¡æ”¹è¿›çš„é¢˜ç›®
        improved = sum(1 for p in self.problems if p.get('improved', False))
        
        # å¹³å‡é—®é¢˜é•¿åº¦
        avg_problem_length = np.mean([len(p.get('problem', '')) for p in self.problems])
        
        # å¹³å‡è§£ç­”æ­¥éª¤æ•°
        avg_solution_steps = 0
        if with_solution > 0:
            total_steps = sum(
                len(p['solution'].get('steps', [])) 
                for p in self.problems 
                if 'solution' in p and 'steps' in p['solution']
            )
            avg_solution_steps = total_steps / with_solution
        
        return {
            'total_problems': total,
            'with_solution': with_solution,
            'solution_rate': with_solution / total if total > 0 else 0,
            'improved_count': improved,
            'improvement_rate': improved / total if total > 0 else 0,
            'avg_problem_length': avg_problem_length,
            'avg_solution_steps': avg_solution_steps,
        }
    
    def calculate_diversity_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡"""
        # æå–æ‰€æœ‰é—®é¢˜æ–‡æœ¬
        problem_texts = [p.get('problem', '') for p in self.problems]
        
        if len(problem_texts) < 2:
            return {'error': 'Not enough problems for diversity calculation'}
        
        # ä½¿ç”¨TF-IDFè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        vectorizer = TfidfVectorizer(max_features=100)
        try:
            tfidf_matrix = vectorizer.fit_transform(problem_texts)
            
            # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
            n = len(problem_texts)
            total_similarity = (similarity_matrix.sum() - n) / (n * (n - 1))
            
            # è®¡ç®—æœ€å¤§å’Œæœ€å°ç›¸ä¼¼åº¦
            upper_triangle = similarity_matrix[np.triu_indices(n, k=1)]
            max_similarity = upper_triangle.max()
            min_similarity = upper_triangle.min()
            
            # å¤šæ ·æ€§åˆ†æ•°ï¼ˆ1 - å¹³å‡ç›¸ä¼¼åº¦ï¼‰
            diversity_score = 1 - total_similarity
            
        except Exception as e:
            return {'error': f'TF-IDF calculation failed: {str(e)}'}
        
        # è¯æ±‡å¤šæ ·æ€§
        all_words = []
        for text in problem_texts:
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            all_words.extend(words)
        
        unique_words = len(set(all_words))
        total_words = len(all_words)
        lexical_diversity = unique_words / total_words if total_words > 0 else 0
        
        return {
            'avg_similarity': total_similarity,
            'max_similarity': max_similarity,
            'min_similarity': min_similarity,
            'diversity_score': diversity_score,
            'lexical_diversity': lexical_diversity,
            'unique_words': unique_words,
            'total_words': total_words,
        }
    
    def analyze_difficulty_distribution(self) -> Dict[str, Any]:
        """åˆ†æéš¾åº¦åˆ†å¸ƒ"""
        difficulties = [p.get('difficulty', 0) for p in self.problems if 'difficulty' in p]
        
        if not difficulties:
            return {'error': 'No difficulty information'}
        
        return {
            'mean': np.mean(difficulties),
            'median': np.median(difficulties),
            'std': np.std(difficulties),
            'min': min(difficulties),
            'max': max(difficulties),
            'distribution': dict(Counter(difficulties)),
            'in_aime_range': sum(1 for d in difficulties if 6 <= d <= 9),
            'aime_range_rate': sum(1 for d in difficulties if 6 <= d <= 9) / len(difficulties),
        }
    
    def analyze_topic_coverage(self) -> Dict[str, Any]:
        """åˆ†æä¸»é¢˜è¦†ç›–"""
        topics = [p.get('topic', 'Unknown') for p in self.problems]
        topic_counts = Counter(topics)
        
        # æ ‡ç­¾ç»Ÿè®¡
        all_tags = []
        for p in self.problems:
            if 'tags' in p:
                all_tags.extend(p['tags'])
        
        tag_counts = Counter(all_tags)
        
        # è®¡ç®—ä¸»é¢˜å‡è¡¡åº¦ï¼ˆç†µï¼‰
        total = len(topics)
        topic_entropy = 0
        for count in topic_counts.values():
            p = count / total
            topic_entropy -= p * np.log2(p) if p > 0 else 0
        
        # æœ€å¤§ç†µï¼ˆå®Œå…¨å‡è¡¡ï¼‰
        max_entropy = np.log2(len(topic_counts)) if len(topic_counts) > 0 else 1
        balance_score = topic_entropy / max_entropy if max_entropy > 0 else 0
        
        return {
            'topic_distribution': dict(topic_counts),
            'unique_topics': len(topic_counts),
            'topic_entropy': topic_entropy,
            'balance_score': balance_score,
            'tag_distribution': dict(tag_counts.most_common(10)),
            'unique_tags': len(tag_counts),
        }
    
    def analyze_answer_distribution(self) -> Dict[str, Any]:
        """åˆ†æç­”æ¡ˆåˆ†å¸ƒ"""
        answers = [p.get('answer', -1) for p in self.problems if 'answer' in p]
        
        if not answers:
            return {'error': 'No answer information'}
        
        # æ£€æŸ¥ç­”æ¡ˆèŒƒå›´
        valid_answers = [a for a in answers if 0 <= a <= 999]
        
        # ç­”æ¡ˆåˆ†å¸ƒç»Ÿè®¡
        answer_ranges = {
            '0-99': sum(1 for a in valid_answers if 0 <= a < 100),
            '100-199': sum(1 for a in valid_answers if 100 <= a < 200),
            '200-299': sum(1 for a in valid_answers if 200 <= a < 300),
            '300-499': sum(1 for a in valid_answers if 300 <= a < 500),
            '500-999': sum(1 for a in valid_answers if 500 <= a <= 999),
        }
        
        return {
            'total_answers': len(answers),
            'valid_answers': len(valid_answers),
            'validity_rate': len(valid_answers) / len(answers) if answers else 0,
            'mean': np.mean(valid_answers) if valid_answers else 0,
            'median': np.median(valid_answers) if valid_answers else 0,
            'std': np.std(valid_answers) if valid_answers else 0,
            'range_distribution': answer_ranges,
        }
    
    def analyze_solution_quality(self) -> Dict[str, Any]:
        """åˆ†æè§£ç­”è´¨é‡"""
        solutions = [p['solution'] for p in self.problems if 'solution' in p and p['solution']]
        
        if not solutions:
            return {'error': 'No solution information'}
        
        # è§£ç­”æ­¥éª¤æ•°ç»Ÿè®¡
        step_counts = [len(s.get('steps', [])) for s in solutions]
        
        # è§£ç­”é•¿åº¦ç»Ÿè®¡
        solution_lengths = []
        for s in solutions:
            total_length = 0
            for step in s.get('steps', []):
                if 'description' in step:
                    total_length += len(step['description'])
            solution_lengths.append(total_length)
        
        # è§£ç­”å®Œæ•´æ€§ï¼ˆæœ‰final_answerçš„æ¯”ä¾‹ï¼‰
        with_final_answer = sum(1 for s in solutions if 'final_answer' in s)
        
        return {
            'total_solutions': len(solutions),
            'avg_steps': np.mean(step_counts) if step_counts else 0,
            'median_steps': np.median(step_counts) if step_counts else 0,
            'min_steps': min(step_counts) if step_counts else 0,
            'max_steps': max(step_counts) if step_counts else 0,
            'avg_length': np.mean(solution_lengths) if solution_lengths else 0,
            'with_final_answer': with_final_answer,
            'completeness_rate': with_final_answer / len(solutions) if solutions else 0,
        }
    
    def calculate_similarity_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆæ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„é¢˜ç›®å¯¹ï¼‰"""
        problem_texts = [p.get('problem', '') for p in self.problems]
        
        if len(problem_texts) < 2:
            return {'error': 'Not enough problems'}
        
        try:
            vectorizer = TfidfVectorizer(max_features=100)
            tfidf_matrix = vectorizer.fit_transform(problem_texts)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            n = len(problem_texts)
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„é¢˜ç›®å¯¹
            max_sim = -1
            max_pair = (0, 0)
            for i in range(n):
                for j in range(i + 1, n):
                    if similarity_matrix[i, j] > max_sim:
                        max_sim = similarity_matrix[i, j]
                        max_pair = (i, j)
            
            # æ‰¾å‡ºæœ€ä¸ç›¸ä¼¼çš„é¢˜ç›®å¯¹
            min_sim = 2
            min_pair = (0, 0)
            for i in range(n):
                for j in range(i + 1, n):
                    if similarity_matrix[i, j] < min_sim:
                        min_sim = similarity_matrix[i, j]
                        min_pair = (i, j)
            
            return {
                'most_similar_pair': {
                    'indices': max_pair,
                    'similarity': max_sim,
                    'problem_1_id': self.problems[max_pair[0]].get('id', 'unknown'),
                    'problem_2_id': self.problems[max_pair[1]].get('id', 'unknown'),
                },
                'least_similar_pair': {
                    'indices': min_pair,
                    'similarity': min_sim,
                    'problem_1_id': self.problems[min_pair[0]].get('id', 'unknown'),
                    'problem_2_id': self.problems[min_pair[1]].get('id', 'unknown'),
                }
            }
        except Exception as e:
            return {'error': f'Similarity calculation failed: {str(e)}'}
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        metrics = self.calculate_all_metrics()
        
        if not metrics:
            return "âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šæ²¡æœ‰æ•°æ®"
        
        report = []
        report.append("=" * 60)
        report.append("AIMEé¢˜ç›®è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 60)
        
        # åŸºæœ¬ç»Ÿè®¡
        if 'basic_stats' in metrics:
            stats = metrics['basic_stats']
            report.append("\nğŸ“Š åŸºæœ¬ç»Ÿè®¡")
            report.append(f"  æ€»é¢˜ç›®æ•°: {stats['total_problems']}")
            report.append(f"  å¸¦è§£ç­”: {stats['with_solution']} ({stats['solution_rate']*100:.1f}%)")
            report.append(f"  å·²æ”¹è¿›: {stats['improved_count']} ({stats['improvement_rate']*100:.1f}%)")
            report.append(f"  å¹³å‡é—®é¢˜é•¿åº¦: {stats['avg_problem_length']:.0f} å­—ç¬¦")
            report.append(f"  å¹³å‡è§£ç­”æ­¥éª¤: {stats['avg_solution_steps']:.1f} æ­¥")
        
        # å¤šæ ·æ€§æŒ‡æ ‡
        if 'diversity' in metrics and 'error' not in metrics['diversity']:
            div = metrics['diversity']
            report.append("\nğŸ¨ å¤šæ ·æ€§æŒ‡æ ‡")
            report.append(f"  å¤šæ ·æ€§åˆ†æ•°: {div['diversity_score']:.3f} (è¶Šé«˜è¶Šå¥½)")
            report.append(f"  å¹³å‡ç›¸ä¼¼åº¦: {div['avg_similarity']:.3f}")
            report.append(f"  è¯æ±‡å¤šæ ·æ€§: {div['lexical_diversity']:.3f}")
            report.append(f"  ç‹¬ç‰¹è¯æ±‡: {div['unique_words']}")
        
        # éš¾åº¦åˆ†å¸ƒ
        if 'difficulty' in metrics and 'error' not in metrics['difficulty']:
            diff = metrics['difficulty']
            report.append("\nğŸ“ˆ éš¾åº¦åˆ†å¸ƒ")
            report.append(f"  å¹³å‡éš¾åº¦: {diff['mean']:.2f}/15")
            report.append(f"  ä¸­ä½æ•°: {diff['median']:.1f}")
            report.append(f"  æ ‡å‡†å·®: {diff['std']:.2f}")
            report.append(f"  AIMEèŒƒå›´(6-9): {diff['in_aime_range']} ({diff['aime_range_rate']*100:.1f}%)")
        
        # ä¸»é¢˜è¦†ç›–
        if 'topic_coverage' in metrics:
            topic = metrics['topic_coverage']
            report.append("\nğŸ¯ ä¸»é¢˜è¦†ç›–")
            report.append(f"  ç‹¬ç‰¹ä¸»é¢˜: {topic['unique_topics']}")
            report.append(f"  ä¸»é¢˜å‡è¡¡åº¦: {topic['balance_score']:.3f} (è¶Šé«˜è¶Šå‡è¡¡)")
            report.append("  ä¸»é¢˜åˆ†å¸ƒ:")
            for t, count in topic['topic_distribution'].items():
                report.append(f"    - {t}: {count}")
        
        # ç­”æ¡ˆåˆ†å¸ƒ
        if 'answer_distribution' in metrics and 'error' not in metrics['answer_distribution']:
            ans = metrics['answer_distribution']
            report.append("\nğŸ² ç­”æ¡ˆåˆ†å¸ƒ")
            report.append(f"  æœ‰æ•ˆç­”æ¡ˆç‡: {ans['validity_rate']*100:.1f}%")
            report.append(f"  å¹³å‡ç­”æ¡ˆ: {ans['mean']:.1f}")
            report.append(f"  ä¸­ä½æ•°: {ans['median']:.1f}")
        
        # è§£ç­”è´¨é‡
        if 'solution_quality' in metrics and 'error' not in metrics['solution_quality']:
            sol = metrics['solution_quality']
            report.append("\nğŸ’¡ è§£ç­”è´¨é‡")
            report.append(f"  å¹³å‡æ­¥éª¤æ•°: {sol['avg_steps']:.1f}")
            report.append(f"  å¹³å‡é•¿åº¦: {sol['avg_length']:.0f} å­—ç¬¦")
            report.append(f"  å®Œæ•´æ€§: {sol['completeness_rate']*100:.1f}%")
        
        report.append("\n" + "=" * 60)
        
        return "\n".join(report)
    
    def save_metrics(self, output_file: str = "evaluation/quality_metrics.json"):
        """ä¿å­˜æŒ‡æ ‡åˆ°JSONæ–‡ä»¶"""
        metrics = self.calculate_all_metrics()
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è´¨é‡æŒ‡æ ‡å·²ä¿å­˜: {output_path}")
        
        return metrics


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ” AIMEé¢˜ç›®è´¨é‡è¯„ä¼°")
    
    calculator = QualityMetrics()
    
    if not calculator.problems:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°é¢˜ç›®æ•°æ®")
        return
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºæŠ¥å‘Š
    report = calculator.generate_summary_report()
    print(report)
    
    # ä¿å­˜æŒ‡æ ‡
    calculator.save_metrics()
    
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()

